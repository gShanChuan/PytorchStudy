{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0707ab57a3f9cf43f3400869693fd2dfcef3fb2a292e08aca37804df915cf11c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# VGG块"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3*3的卷积层后接一个歩幅为2的最大池化层。卷积层保持输入的h和w不变，而池化层使h和w减半。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "def vggBlock(numConvs,inChannels,outChannels):\n",
    "    vggBlock=[]\n",
    "    for i in range(numConvs):\n",
    "        vggBlock.append(nn.Conv2d(inChannels if i==0 else outChannels,outChannels,3,1,1))\n",
    "        vggBlock.append(nn.ReLU())\n",
    "    vggBlock.append(nn.MaxPool2d(2,2))\n",
    "    return nn.Sequential(*vggBlock)"
   ]
  },
  {
   "source": [
    "# VGG网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "卷积层模块串联数个vggBlock，其超参数由变量conv_arch定义。该变量指定了每个VGG块里的卷积层个数和输入输出通道数。全连接模块和AlexNet中的一样"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1,1,64),(1,64,128),(2,128,256),(2,256,512),(2,512,512))\n",
    "fc_features=512*7*7\n",
    "fc_hidden_units=4096"
   ]
  },
  {
   "source": [
    "VGG_11"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch,fc_features,fc_hidden_units=4096):\n",
    "    net = nn.Sequential()\n",
    "    for i ,(numConvs,inChannels,outChannels) in enumerate(conv_arch):\n",
    "        net.add_module(str(i),vggBlock(numConvs,inChannels,outChannels))\n",
    "    net.add_module('fc',nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(fc_features,fc_hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(fc_hidden_units,fc_hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(fc_hidden_units,10)\n",
    "    ))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 torch.Size([1, 64, 112, 112])\n1 torch.Size([1, 128, 56, 56])\n2 torch.Size([1, 256, 28, 28])\n3 torch.Size([1, 512, 14, 14])\n4 torch.Size([1, 512, 7, 7])\nfc torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch,fc_features,fc_hidden_units)\n",
    "x=torch.rand(1,1,224,224)\n",
    "\n",
    "for name,layer in net.named_children():\n",
    "    x=layer(x)\n",
    "    print(name,x.shape)"
   ]
  },
  {
   "source": [
    "因为VGG-11比AlexNet计算上更加复杂，出于测试目的，我们构造一个通道数更小，或者说更窄的网络在FashionMNIST数据集上进行测试。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (1): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (4): Sequential(\n    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Flatten()\n    (1): Linear(in_features=3136, out_features=4096, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.5, inplace=False)\n    (7): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "ratio=8\n",
    "small_conv_arch = ((1,1,64//ratio),(1,64//ratio,128//ratio),(2,128//ratio,256//ratio),(2,256//ratio,512//ratio),(2,512//ratio,512//ratio))\n",
    "net=vgg(small_conv_arch,fc_features//8,fc_hidden_units)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}