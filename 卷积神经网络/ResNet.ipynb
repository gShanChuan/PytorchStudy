{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "ResNet沿用了VGG全3*3卷积层的设计。  \n",
    "残差块首先有2个有相同输出通道数的3\\*3卷积层。每个卷积层后接一个批量归一化层和relu激活函数。  \n",
    "然后将输入跳过这两个卷积运算后直接加在最后的Relu函数前。  \n",
    "这样的设计要求输出的与输入的形状形状一样，从而可以相加：  \n",
    "1.通道数不相同->使用1\\*1卷积层改变  \n",
    "2.空间尺寸不同->引入相同的线性映射"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self,inChannels,outChannels,use_1x1conv=False,stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv2d(inChannels,outChannels,3,stride,1),\n",
    "            nn.BatchNorm2d(outChannels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(outChannels,outChannels,3,padding=1),\n",
    "            nn.BatchNorm2d(outChannels)\n",
    "        )\n",
    "        if use_1x1conv:\n",
    "            self.conv3=nn.Conv2d(inChannels,outChannels,1,stride=stride)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "    def forward(self,input):\n",
    "        outputConv1=self.conv1(input)\n",
    "        outputConv2=self.conv2(outputConv1)\n",
    "        if self.conv3:\n",
    "            input=self.conv3(input)\n",
    "        return f.relu(outputConv2+input) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "X = torch.rand((4, 3, 6, 6))\n",
    "blk(X).shape # torch.Size([4, 3, 6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "blk = Residual(3, 6, use_1x1conv=True, stride=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "source": [
    "# ResNet模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(\n",
    "    nn.Conv2d(1,64,7,2,3),\n",
    "    nn.BatchNorm2d(64), \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, 2, 1)\n",
    ")"
   ]
  },
  {
   "source": [
    "GoogLeNet在后面接了4个由Inception块组成的模块。  \n",
    "ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。  \n",
    "第一个模块的通道数同输入通道数一致。  \n",
    "由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnetBlock(inChannels,outChannels,numResiduals,first_block=False):\n",
    "    if first_block :\n",
    "        assert inChannels == outChannels\n",
    "    blk = []\n",
    "    for i in range(numResiduals):\n",
    "        if i==0 and not first_block:\n",
    "            blk.append(Residual(inChannels, outChannels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(outChannels, outChannels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_module(\"resnet_block1\", resnetBlock(64, 64, 2, first_block=True))\n",
    "net.add_module(\"resnet_block2\", resnetBlock(64, 128, 2))\n",
    "net.add_module(\"resnet_block3\", resnetBlock(128, 256, 2))\n",
    "net.add_module(\"resnet_block4\", resnetBlock(256, 512, 2))"
   ]
  },
  {
   "source": [
    "与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return torch.nn.functional.adaptive_avg_pool2d(x, (1,1))\n",
    "net.add_module(\"global_avg_pool\", GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\n",
    "net.add_module(\"fc\", nn.Sequential(nn.Flatten(), nn.Linear(512, 10))) "
   ]
  },
  {
   "source": [
    "观察输入形状变化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 112, 112])\n1  output shape:\t torch.Size([1, 64, 112, 112])\n2  output shape:\t torch.Size([1, 64, 112, 112])\n3  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\nresnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\nresnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\nglobal_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 224, 224))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)"
   ]
  },
  {
   "source": [
    "获取数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "#获取数据集\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())\n",
    "#读取数据集\n",
    "batchSize=256\n",
    "trainIter=torch.utils.data.DataLoader(mnist_train,batch_size=batchSize,shuffle=True,num_workers=8)\n",
    "testIter=torch.utils.data.DataLoader(mnist_test,batch_size=batchSize,shuffle=True,num_workers=8)"
   ]
  },
  {
   "source": [
    "评价"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    device=torch.device('cuda')\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        net.eval() # 评估模式\n",
    "        for X, y in data_iter:\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "        net.train() # 改回训练模式 \n",
    "    return acc_sum / n"
   ]
  },
  {
   "source": [
    "训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.0017218314222991467 0.8382666666666667 0.8006\n",
      "1 0.0011902463374038538 0.88765 0.8732\n",
      "2 0.001012880956629912 0.9041833333333333 0.8889\n",
      "3 0.0009097160746653875 0.9146666666666666 0.9003\n",
      "4 0.0008314816568046809 0.92055 0.8758\n",
      "5 0.0007617413983990748 0.9264 0.8972\n",
      "6 0.0007065044568230709 0.9327833333333333 0.8932\n",
      "7 0.0006406198677917322 0.93815 0.8954\n",
      "8 0.00059957183363537 0.9425833333333333 0.9017\n",
      "9 0.0005423044926176468 0.9472833333333334 0.903\n"
     ]
    }
   ],
   "source": [
    "lr,epochsNum = 0.001,10\n",
    "net=net.cuda()\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimzer=torch.optim.Adam(net.parameters(),lr)\n",
    "\n",
    "for epoch in range(epochsNum):\n",
    "    train_l_sum=n=train_acc_sum=0\n",
    "    for x,y in trainIter:\n",
    "        x=x.cuda()\n",
    "        y=y.cuda()\n",
    "        yP=net(x)\n",
    "        l=loss(yP,y)\n",
    "        l.backward()\n",
    "        optimzer.step()\n",
    "        optimzer.zero_grad()\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (yP.argmax(dim=1) == y).sum().item()\n",
    "        n +=y.shape[0]\n",
    "    print(epoch,train_l_sum/n,train_acc_sum/n,evaluate_accuracy(testIter,net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}