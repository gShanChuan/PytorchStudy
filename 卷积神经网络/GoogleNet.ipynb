{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0707ab57a3f9cf43f3400869693fd2dfcef3fb2a292e08aca37804df915cf11c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# GoogleNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Inception块"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Inception块里有4条并行的线路。  \n",
    "前3条线路使用窗口大小分别是1×1、3×3和5×5的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1×11×1卷积来减少输入通道数，以降低模型复杂度。  \n",
    "第四条线路则使用3×3最大池化层，后接1×1卷积层来改变通道数。  \n",
    "4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self,inc,c1,c2,c3,c4):\n",
    "        super().__init__()\n",
    "        #线路1 单1 x 1卷积层\n",
    "        self.p1=nn.Sequential(\n",
    "            nn.Conv2d(inc,c1,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #线路2 1 x 1卷积层后接3 x 3卷积层\n",
    "        self.p2=nn.Sequential(\n",
    "             nn.Conv2d(inc, c2[0], 1),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(c2[0],c2[1], 3, padding=1),\n",
    "             nn.ReLU()\n",
    "        )\n",
    "        #线路3 1 x 1卷积层后接5 x 5卷积层\n",
    "        self.p3=nn.Sequential(\n",
    "            nn.Conv2d(inc, c3[0], 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c3[0],c3[1],5,padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #线路4 3 x 3最大池化层后接1 x 1卷积层\n",
    "        self.p4=nn.Sequential(\n",
    "            nn.MaxPool2d(3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(inc,c4,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        p1=self.p1(input)\n",
    "        p2=self.p2(input)\n",
    "        p3=self.p3(input)\n",
    "        p4=self.p4(input)\n",
    "\n",
    "        return torch.cat((p1,p2,p3,p4),dim=1)\n"
   ]
  },
  {
   "source": [
    "# GoogleNet模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "GooglNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用歩幅为2的3*3的最大池化层来减小输出高宽"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "第一个模块使用64通道的7*7卷积层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=nn.Sequential(\n",
    "    nn.Conv2d(1,64,7,2,3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3,3,1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第二个模块使用2个卷积层：首先是64通道的1*1卷积层  \n",
    "然后将通道数增大3倍的3*3卷积层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=nn.Sequential(\n",
    "    nn.Conv2d(64,64,1),\n",
    "    nn.Conv2d(64,192,3,padding=1),\n",
    "    nn.MaxPool2d(3,2,1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第三模块串联2个完整的Inception块。  \n",
    "第一个Inception块的输出通道数为64+128+32+32=25664+128+32+32=256，其中4条线路的输出通道数比例为64:128:32:32=2:4:1:164:128:32:32=2:4:1:1。其中第二、第三条线路先分别将输入通道数减小至96/192=1/296/192=1/2和16/192=1/1216/192=1/12后，再接上第二层卷积层。  \n",
    "第二个Inception块输出通道数增至128+192+96+64=480128+192+96+64=480，每条线路的输出通道数之比为128:192:96:64=4:6:3:2128:192:96:64=4:6:3:2。其中第二、第三条线路先分别将输入通道数减小至128/256=1/2128/256=1/2和32/256=1/832/256=1/8。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "source": [
    "第四模块更加复杂。  \n",
    "它串联了5个Inception块，其输出通道数分别是192+208+48+64=512192+208+48+64=512、160+224+64+64=512160+224+64+64=512、128+256+64+64=512128+256+64+64=512、112+288+64+64=528112+288+64+64=528和256+320+128+128=832256+320+128+128=832。这些线路的通道数分配和第三模块中的类似，首先含3×3卷积层的第二条线路输出最多通道，其次是仅含1×1卷积层的第一条线路，之后是含5×5卷积层的第三条线路和含3×3最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "source": [
    "第五模块有输出通道数为256+320+128+128=832256+320+128+128=832和384+384+128+128=1024384+384+128+128=1024的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return torch.nn.functional.adaptive_avg_pool2d(x, (1,1))\n",
    "\n",
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   GlobalAvgPool2d())\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5, \n",
    "                    nn.Flatten(), nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 torch.Size([1, 64, 16, 16])\n1 torch.Size([1, 192, 8, 8])\n2 torch.Size([1, 480, 4, 4])\n3 torch.Size([1, 832, 2, 2])\n4 torch.Size([1, 1024, 1, 1])\n5 torch.Size([1, 1024])\n6 torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 1, 96, 96)\n",
    "for name,blk in net.named_children():\n",
    "    x=blk(x)\n",
    "    print(name,x.shape)"
   ]
  },
  {
   "source": [
    "# 获取数据集"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Resize(size=96),\n",
    "    torchvision.transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "#获取数据集\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transform)\n",
    "#读取数据集\n",
    "batchSize=128\n",
    "trainIter=torch.utils.data.DataLoader(mnist_train,batch_size=batchSize,shuffle=True,num_workers=8)\n",
    "testIter=torch.utils.data.DataLoader(mnist_test,batch_size=batchSize,shuffle=True,num_workers=8)\n"
   ]
  },
  {
   "source": [
    "# 评价"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    device=torch.device('cuda')\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        net.eval() # 评估模式\n",
    "        for X, y in data_iter:\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "        net.train() # 改回训练模式 \n",
    "    return acc_sum / n"
   ]
  },
  {
   "source": [
    "# 训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.008870575345059237 0.5308166666666667 0.8011\n",
      "1 0.0034264414305488267 0.8415166666666667 0.8573\n",
      "2 0.0027164866690834364 0.87275 0.8683\n",
      "3 0.0023870243221521376 0.8875666666666666 0.8899\n",
      "4 0.0020869226388633253 0.9016833333333333 0.8904\n"
     ]
    }
   ],
   "source": [
    "lr,epochsNum = 0.001,5\n",
    "net=net.cuda()\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimzer=torch.optim.Adam(net.parameters(),lr)\n",
    "\n",
    "for epoch in range(epochsNum):\n",
    "    train_l_sum=n=train_acc_sum=0\n",
    "    for x,y in trainIter:\n",
    "        x=x.cuda()\n",
    "        y=y.cuda()\n",
    "        yP=net(x)\n",
    "        l=loss(yP,y)\n",
    "        l.backward() \n",
    "        optimzer.step()\n",
    "        optimzer.zero_grad()\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (yP.argmax(dim=1) == y).sum().item()\n",
    "        n +=y.shape[0]\n",
    "    print(epoch,train_l_sum/n,train_acc_sum/n,evaluate_accuracy(testIter,net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}