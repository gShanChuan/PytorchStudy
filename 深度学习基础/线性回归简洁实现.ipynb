{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0707ab57a3f9cf43f3400869693fd2dfcef3fb2a292e08aca37804df915cf11c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "导入包及模块"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "生成数据集"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=2\n",
    "num_examples=1000\n",
    "true_w=[2,-2.3]\n",
    "true_b=4.2\n",
    "\n",
    "features=torch.randn(num_examples,num_inputs,dtype=torch.float32)\n",
    "labels = true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b\n",
    "labels += torch.tensor(np.random.normal(0,0.001),dtype=torch.float32)"
   ]
  },
  {
   "source": [
    "读取数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "batchSize=10\n",
    "dataset=data.TensorDataset(features,labels)\n",
    "#随机读取小批量\n",
    "dataIter=data.DataLoader(dataset,batchSize,shuffle=True)"
   ]
  },
  {
   "source": [
    "定义模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class linearNet(nn.Module):\n",
    "    def __init__(self,featureNum):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(featureNum,1)\n",
    "    def forward(self,x):\n",
    "        y=self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "source": [
    "查看模型结构"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "linearNet(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net = linearNet(num_inputs)\n",
    "print(net) "
   ]
  },
  {
   "source": [
    "其他定义模型结构的方式"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\nLinear(in_features=2, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "#写法1\n",
    "net=nn.Sequential(\n",
    "    nn.Linear(num_inputs,1)\n",
    ")\n",
    "#写法2\n",
    "net=nn.Sequential()\n",
    "net.add_module('linear',nn.Linear(num_inputs,1))\n",
    "#写法3\n",
    "from collections import OrderedDict\n",
    "net=nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            ('linear',nn.Linear(num_inputs,1))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(net)\n",
    "print(net[0])"
   ]
  },
  {
   "source": [
    "查看模型参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[-0.5777,  0.1922]], requires_grad=True)\nParameter containing:\ntensor([0.2188], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#net.parameters()查看模型所有的可学习参数\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "source": [
    "初始化模型参数  \n",
    "__init模块中提供了多种参数初始化方法___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net[0].weight,mean=0,std=0.01)\n",
    "init.constant_(net[0].bias,val=0)"
   ]
  },
  {
   "source": [
    "定义损失函数  \n",
    "__PyTorch在nn模块中提供了各种损失函数__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#均方误差\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "source": [
    "定义优化算法  \n",
    "__torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.03\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n"
     ]
    }
   ],
   "source": [
    "# 小批量随机梯队下降优化算法\n",
    "import torch.optim as optim\n",
    "optimzer = optim.SGD(net.parameters(),lr=0.03)\n",
    "print(optimzer)"
   ]
  },
  {
   "source": [
    "还可以为子网络设置不同的学习率"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleAttributeError",
     "evalue": "'Sequential' object has no attribute 'subnet1'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-dde4e7447a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer =optim.SGD([\n\u001b[1;32m      2\u001b[0m                 \u001b[0;31m# 如果对某个参数不指定学习率，就使用最外层的默认学习率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                 \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# lr=0.03\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubnet2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             ], lr=0.03)\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'Sequential' object has no attribute 'subnet1'"
     ]
    }
   ],
   "source": [
    "optimizer =optim.SGD([\n",
    "                # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "                {'params': net.subnet1.parameters()}, # lr=0.03\n",
    "                {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "            ], lr=0.03)"
   ]
  },
  {
   "source": [
    "有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。  \n",
    "__修改optimizer.param_groups中对应的学习率__  \n",
    "__新建优化器__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "#调整学习率\n",
    "for param in optimzer.param_groups:\n",
    "    print(param['lr'])"
   ]
  },
  {
   "source": [
    "训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 7.643397199885715e-12\n1 7.790390728346086e-12\n2 7.754863591558081e-12\n"
     ]
    }
   ],
   "source": [
    "epochsNum=3\n",
    "for epoch in range(epochsNum):\n",
    "    for x,y in dataIter:\n",
    "        output=net(x)\n",
    "        l=loss(output,y.view(-1,1))\n",
    "        l.backward()\n",
    "        optimzer.step()\n",
    "        optimzer.zero_grad()\n",
    "    \n",
    "    print(epoch,l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2, -2.3] Parameter containing:\ntensor([[ 2.0000, -2.3000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w,net[0].weight)"
   ]
  }
 ]
}