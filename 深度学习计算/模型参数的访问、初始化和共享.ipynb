{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch1.6",
   "display_name": "Python [conda env:pytorch1.6]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=4, out_features=3, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=3, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np \n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,3),nn.ReLU(),nn.Linear(3,1))\n",
    "\n",
    "print(net)\n",
    "x=torch.rand(2,4)\n",
    "y=net(x).sum()"
   ]
  },
  {
   "source": [
    "# 访问模型参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 通过Module类的parameters()\n",
    "* 或者named_parameters()  \n",
    "来访问所有参数（上述均以迭代器的形式返回）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'generator'>\n",
      "<class 'generator'>\n",
      "0.weight Parameter containing:\n",
      "tensor([[ 0.1741, -0.1956, -0.2257,  0.0487],\n",
      "        [-0.2008,  0.3406, -0.3661,  0.1318],\n",
      "        [ 0.0457,  0.2257,  0.3323, -0.2242]], requires_grad=True)\n",
      "0.bias Parameter containing:\n",
      "tensor([-0.1013, -0.4943,  0.3208], requires_grad=True)\n",
      "2.weight Parameter containing:\n",
      "tensor([[-0.1511,  0.2940, -0.1841]], requires_grad=True)\n",
      "2.bias Parameter containing:\n",
      "tensor([0.2517], requires_grad=True)\n",
      "**************************************\n",
      "**************************************\n",
      "**************************************\n",
      "**************************************\n",
      "Parameter containing:\n",
      "tensor([[ 0.1741, -0.1956, -0.2257,  0.0487],\n",
      "        [-0.2008,  0.3406, -0.3661,  0.1318],\n",
      "        [ 0.0457,  0.2257,  0.3323, -0.2242]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1013, -0.4943,  0.3208], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1511,  0.2940, -0.1841]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2517], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "print(type(net.parameters()))\n",
    "for name,param in net.named_parameters():\n",
    "    print(name,param)\n",
    "print('**************************************')\n",
    "print('**************************************')\n",
    "print('**************************************')\n",
    "print('**************************************')\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "source": [
    "可以看出返回的名字自动加上了层数的索引作为前缀。我们再来访问net中单层的参数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\nbias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for name,param in net[0].named_parameters():\n",
    "    print(name,param.shape,type(param))"
   ]
  },
  {
   "source": [
    "返回的param类型为torch.nn.parameter.Parameter，其实是torch.tensor的子类，和tensor__不同的是如果一个tensor是parameter，那么他会自动添加到模型的参数列表中。__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight1 = nn.Parameter(torch.rand(20,20))\n",
    "        self.weight2 = torch.tensor(np.random.rand(20,20))\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "n = MyModel()\n",
    "for name, param in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "source": [
    "上述代码中weight1在参数列表中，但是weight2不在参数列表中"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "parameter是tensor，即tensor拥有的属性它都有，比如可以根据data访问参数值，用grad访问参数梯度"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[ 0.1741, -0.1956, -0.2257,  0.0487],\n        [-0.2008,  0.3406, -0.3661,  0.1318],\n        [ 0.0457,  0.2257,  0.3323, -0.2242]], requires_grad=True)\nNone\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.2413, -0.1270, -0.1922, -0.1798]])\n"
     ]
    }
   ],
   "source": [
    "weight0=list(net[0].parameters())[0]\n",
    "print(weight0)\n",
    "#反向传播前梯度\n",
    "print(weight0.grad)\n",
    "y.backward()\n",
    "print(weight0.grad)"
   ]
  },
  {
   "source": [
    "# 初始化模型参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pytorch中nn.Module模块参数采用了较为合理的初始化策略"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "但是我们经常需要使用其他方法来初始化权重。pytorch的__init模块__中提供了多种预设的初始化方法。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.weight Parameter containing:\n",
      "tensor([[ 0.0070,  0.0085,  0.0091, -0.0145],\n",
      "        [ 0.0023, -0.0060, -0.0078, -0.0120],\n",
      "        [ 0.0153,  0.0084,  0.0156, -0.0154]], requires_grad=True)\n",
      "2.weight Parameter containing:\n",
      "tensor([[-0.0080, -0.0148,  0.0040]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param,0,0.01)\n",
    "        print(name,param)"
   ]
  },
  {
   "source": [
    "下面使用常数来初始化权重参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.bias Parameter containing:\ntensor([0., 0., 0.], requires_grad=True)\n2.bias Parameter containing:\ntensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param,val=0)\n",
    "        print(name,param)"
   ]
  },
  {
   "source": [
    "# 自定义初始化方法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "inplace改变Tensor值+不记录梯度"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.weight tensor([[-9.5132, -9.1935,  0.0000,  0.0000],\n        [ 6.2710,  9.5009, -6.4043, -9.1866],\n        [ 7.6908, -9.0243, -9.0246,  5.0344]])\n2.weight tensor([[-9.6534, -8.5862, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def initWeight(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10,10)\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        initWeight(param)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "source": [
    "还可以改变这些参数的data来改写模型参数值同时不影响梯度"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 共享模型参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "有些情况下，我们希望在各个层之间共享参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=1, out_features=1, bias=False)\n  (1): Linear(in_features=1, out_features=1, bias=False)\n)\n0.weight Parameter containing:\ntensor([[3.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear=nn.Linear(1,1,bias=False)\n",
    "net=nn.Sequential(linear,linear)\n",
    "print(net)\n",
    "for name,param in net.named_parameters():\n",
    "    init.constant_(param,val=3)\n",
    "    print(name,param)"
   ]
  },
  {
   "source": [
    "在内存中，这两个线性层其实是一个对象"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0])==id(net[1]))\n",
    "print(id(net[0].weight)==id(net[1].weight))"
   ]
  },
  {
   "source": [
    "因为模型参数里包含了梯度，再反向传播时,这些参数的梯度是累加的"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[6.]])\ntensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(1,1)\n",
    "y=net(x)\n",
    "y.backward()\n",
    "print(net[0].weight.grad)\n",
    "print(net[1].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}