{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch1.6",
   "display_name": "Python [conda env:pytorch1.6]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 继承Moudle类来构造模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Moudle类是nn模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的结构。需要重载Moudle类的__init__函数及forward函数。分别是用于创建模型参数和定义前向计算。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output=nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        a=self.act(self.hidden(x))\n",
    "        return self.output(a)\n"
   ]
  },
  {
   "source": [
    "实例化MLP类得到模型变量net"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLP(\n  (hidden): Linear(in_features=784, out_features=256, bias=True)\n  (act): ReLU()\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net=MLP()\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "输入数据X做一次前向计算。其中，net(X)会调用MLP继承自Module类的__call__函数，这个函数将调用MLP类定义的forward函数来完成前向计算。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0123, -0.1601, -0.1327, -0.1055, -0.0150, -0.0542, -0.0173,  0.0336,\n",
       "          0.1206,  0.1027],\n",
       "        [-0.0837, -0.0960, -0.1623, -0.1244,  0.0833, -0.0436,  0.0164, -0.0550,\n",
       "          0.1342,  0.0580]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "net(torch.rand(2,784))"
   ]
  },
  {
   "source": [
    "这里并没有将Moudle类命名为Layer或model之类的名字，因为该类是一个可供选择自由组建的部件。它的子类既可以是一个层，又可以是一个模型，或者是模型的一部分。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Module的子类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "PyTorch还实现了继承自Module的可以方便构建模型的类: 如Sequential、ModuleList和ModuleDict等等。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Sequential类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "当模型的前向计算为简单串联各个层的计算时，Sequential类可以通过更简单的方式定义模型。  \n",
    "他可以接受一个子模块的有序序列(OrderDict)或者一系列子模块作为参数来逐一添加Module的实例，而模型的前向计算就是按照这些实例的添加顺序逐一计算。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "自己实现Sequential类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        if len(args)==1 and isinstance(args[0],OrderedDict):\n",
    "            for key,module in args[0].items():\n",
    "                self.add_module(key,module)\n",
    "        else:\n",
    "            for index,module in enumerate(args):\n",
    "                self.add_module(str(index),module)\n",
    "    def forward(self,input):\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "source": [
    "用MySequential类来实现前面描述的MLP类，并使用随机初始化的模型做一次前向计算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MySequential(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.2222, -0.2102, -0.0335,  0.1461,  0.0089,  0.0768, -0.0336, -0.0614,\n",
       "         -0.0328,  0.1080],\n",
       "        [ 0.3054, -0.0729, -0.0752,  0.1059,  0.0129,  0.0121,  0.1086, -0.0806,\n",
       "         -0.0863,  0.0763]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "net = MySequential(\n",
    "    nn.Linear(784,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "print(net)\n",
    "net(torch.rand(2,784))"
   ]
  },
  {
   "source": [
    "* ModuleList类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ModuleList接受一个子模块列表作为输入，然后也可以类似List呢样进行append和extend操作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\nModuleList(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleList([nn.Linear(784,256),nn.ReLU()])\n",
    "#append\n",
    "net.append(nn.Linear(256,10))\n",
    "#索引访问\n",
    "print(net[-1])\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "Sequential和ModuleList都可以模块化构造网络。  \n",
    "区别是：  \n",
    "ModuleList仅仅是一个存储各种模块的列表，这些模块间没有联系也没有顺序（所以不用保证相邻层的输入输出匹配），而且没有实现forward方法。  \n",
    "Sequential内的模块会按照顺序排列组合，要保证相邻层的输入输出大小匹配，内部自动实现forward方法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ModuleList的出现只是让网络的前向传播时更加灵活"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linears=nn.ModuleList([nn.Linear(10,10) for i in range(10)])\n",
    "    def forward(self,x):\n",
    "        for i,l in enumerate(self.linears):\n",
    "            x=self.linears[i//2](x)+l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MyModule(\n  (linears): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n    (1): Linear(in_features=10, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n    (3): Linear(in_features=10, out_features=10, bias=True)\n    (4): Linear(in_features=10, out_features=10, bias=True)\n    (5): Linear(in_features=10, out_features=10, bias=True)\n    (6): Linear(in_features=10, out_features=10, bias=True)\n    (7): Linear(in_features=10, out_features=10, bias=True)\n    (8): Linear(in_features=10, out_features=10, bias=True)\n    (9): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(MyModule())"
   ]
  },
  {
   "source": [
    "如上所示，ModuleList不同于一般的Python的List，加入到Modulist里面的所有的模块的参数会自动添加到整个网络中。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Module_ModuleList(\n  (linears): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\nModule_List()\n"
     ]
    }
   ],
   "source": [
    "class Module_ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linears=nn.ModuleList([nn.Linear(10,10)])\n",
    "class Module_List(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linears=[nn.Linear(10,10)]\n",
    "print(Module_ModuleList())\n",
    "print(Module_List())"
   ]
  },
  {
   "source": [
    "* ModuleDict类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ModuleDict接受一个子模块的字典作为输入，然后也可以类似字典呢样进行添加访问操作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\nLinear(in_features=256, out_features=10, bias=True)\nModuleDict(\n  (act): ReLU()\n  (linear): Linear(in_features=784, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net=nn.ModuleDict({\n",
    "    'linear':nn.Linear(784,256),\n",
    "    'act':nn.ReLU()\n",
    "})\n",
    "net['output']=nn.Linear(256,10)\n",
    "print(net['linear'])\n",
    "print(net.output)\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "和ModuleList一样，ModuleDict实例仅仅是存放了一些模块的字典，并没有定义forward函数，同样ModuleDict里的所有模块的参数会被自动添加到整个网路"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 构造复杂的网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "虽然上面介绍的类可以使得模型的构造更加灵活，且不需要forward函数，但是直接继承Module类可以极大的拓展模型构造的灵活性。  \n",
    "我们通过get_constant函数创建训练值不被迭代的参数，即常数参数。  \n",
    "在前向计算中，除了使用创建的常数参数外，我们还要使用Tenso的函数和Python的控制流，并多次调用相同的层。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FancyMLP(\n  (linear): Linear(in_features=20, out_features=20, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-6.8642, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.randWeight=torch.rand((20,20),requires_grad=False)\n",
    "        self.linear=nn.Linear(20,20)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.linear(x)\n",
    "        #使用创建的常参数\n",
    "        x=nn.functional.relu(torch.mm(x,self.randWeight)+1)\n",
    "\n",
    "        #复用全连接层\n",
    "        x=self.linear(x)\n",
    "\n",
    "        #控制流\n",
    "        while x.norm().item() > 1 :\n",
    "            x /= 2\n",
    "        if x.norm().item() < 0.8 :\n",
    "            x*=10\n",
    "        return x.sum()\n",
    "x=torch.rand(2,20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(40,30),nn.ReLU())\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): NestMlp(\n    (net): Sequential(\n      (0): Linear(in_features=40, out_features=30, bias=True)\n      (1): ReLU()\n    )\n  )\n  (1): Linear(in_features=30, out_features=20, bias=True)\n  (2): FancyMLP(\n    (linear): Linear(in_features=20, out_features=20, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(NestMlp(),nn.Linear(30,20),FancyMLP())\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}